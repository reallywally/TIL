# 텍스트 스플리터를 알아보자

## 왜 분할해야 할까?

A라는 회사가 100억 투자를 받았다는 뉴스 기사가 있다고 생각해보자. 이 기사로 RAG를 만들고 "투자받은 금액이 얼마야?" 라는 질문을 했을때 기사에서 실제로 답변에 필요한 부분은 많지 않다. 즉, 질문에 연관성있는 부분만 가져오기 위한 정확성과 효율성을 위해 적정 단위로 텍스트를 분리해야한다.

## 텍스트 스플리터 작업이 어려운 이유

특정 텍스트 스플리터가 너무 좋고 상황에 관계 없이 스플리터에 입력하는 파라미터가 항상 효율적이라면 좋겠지만 그렇지 않다. 좋은 결과를 얻기 위해서는 여러 스플리터를 다양한 값으로 조정해보면서 결과를 확인해야하는 실험(노가다)가 필요하다. 그래서 1개의 스플리터만 알고있는것 보다 다양한 스플리터를 알고 많은 실험을 해볼 수 있는 능력이 중요하다.

## 종류

지금은 각 스플리터마다 자세히 알아보기 보다는 간단하게 이런 종류가 있음을 인지하고 나중에 실제로 사용하게 되었을때 내용을 추가해볼 계획이다.

### CharacterTextSplitter

가장 간단한 방법으로 ```"\n\n"```을 기본값으로 텍스트를 분할한다.

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 텍스트를 분할할 때 사용할 구분자를 지정. 기본값은 "\n\n"
    # separator=" ",
    # 분할된 텍스트 청크의 최대 크기를 지정
    chunk_size=250,
    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정
    chunk_overlap=50,
    # 텍스트의 길이를 계산하는 함수를 지정
    length_function=len,
    # 구분자가 정규식인지 여부를 지정
    is_separator_regex=False,
)
```

### RecursiveCharacterTextSplitter

일반적으로 텍스트 스플리터 사용할때 권장되는 방식이다. 분할 순서는 청크가 추분히 작아질 때까지 ```["\n\n", "\n", " ", ""]``` 순(단락 -> 문장 -> 단어)으로 텍스트를 분할한다.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    # 청크 크기 설정 
    chunk_size=250,
    # 청크 간의 중복되는 문자 수를 설정
    chunk_overlap=50,
    # 문자열 길이를 계산하는 함수를 지정
    length_function=len,
    # 구분자로 정규식을 사용할지 여부를 설정
    is_separator_regex=False,
)
```

### TokenTextSplitter

토큰 수를 기반으로 청크를 생성할때 유용하다.

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    # 청크 크기를 300으로 설정
    chunk_size=300,
    # 청크 간 중복되는 부분 설정
    chunk_overlap=0,
)

texts = text_splitter.split_text(file)
```

### SemanticChunker

텍스트를 의미적 경계를 고려해서 잘라내는 방법이다. 단순 길이, 청킹 사이즈 만큼 자르는 스플리터 대비 맥락이 유지되는 장점이 있다. 청킹되는 예시는 다음과 같다.

```plaintext
[원문]
챗GPT는 OpenAI에서 개발한 대규모 언어 모델이다. 
이 모델은 질문에 대답하고, 글을 요약하거나, 코드를 작성할 수 있다. 
RAG 시스템은 이 모델에 외부 지식을 결합해 성능을 높인다. 
특히 청킹 전략은 검색 품질에 중요한 역할을 한다.

[단순 청킹]
[청크1] 챗GPT는 OpenAI에서 개발한 대규모 언어 모델이다. 이 모델은 질
[청크2] 문에 대답하고, 글을 요약하거나, 코드를 작성할 수 있다. RAG 
[청크3] 시스템은 이 모델에 외부 지식을 결합해 성능을 높인다. 특히 청
[청크4] 킹 전략은 검색 품질에 중요한 역할을 한다.

[의미 단위 청킹]
[청크1] 챗GPT는 OpenAI에서 개발한 대규모 언어 모델이다. 
이 모델은 질문에 대답하고, 글을 요약하거나, 코드를 작성할 수 있다. 
[청크2] RAG 시스템은 이 모델에 외부 지식을 결합해 성능을 높인다. 
특히 청킹 전략은 검색 품질에 중요한 역할을 한다.
```

예시 코드는 다음과 같다.

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.
text_splitter = SemanticChunker(OpenAIEmbeddings())

chunks = text_splitter.split_text(file)
```

### CodeTextSplitter

프로그램 코드 역시 단순 길이로 잘라 ```def add(a, b): return a +```, ```b def multiply(a, b): return a * b``` 이렇게 된다면 검색도 어렵고 LLM도 이해하기 어렵다. 그래서 텍스트를 프로그래밍 언어의 구조적 단위(함수, 클래스 등)를 고려하여 청킹하기위해 CodeTextSplitter를 사용한다. 예시는 파이썬으로 작성되었지만 기타 다양한 언어를 지원한다.

```python
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)

splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=50,
    chunk_overlap=0
)

code = """
def add(a, b):
    return a + b

def multiply(a, b):
    return a * b
"""

python_docs = splitter.create_documents([code])
for doc in python_docs:
    print(doc.page_content, end="\n==================\n")

"""출력 결과
def add(a, b):
    return a + b
==================
def multiply(a, b):
    return a * b
==================
"""
```

### MarkdownHeaderTextSplitter

마크다운 문서를 헤더 구조를 기반으로 청킹한다.

```python
from langchain_text_splitters import (
    MarkdownHeaderTextSplitter,
)

markdown_content = """
# 파이썬 기초

파이썬은 쉽고 강력한 프로그래밍 언어입니다.

## 변수 선언

변수는 다음과 같이 선언합니다:
- name = "홍길동"
- age = 25

## 함수 정의

함수는 def 키워드를 사용합니다:

### 함수 호출
함수는 다음과 같이 호출합니다.
"""

print("=== MarkdownHeaderTextSplitter ===")
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"), 
    ("###", "Header 3")
]
md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
md_header_docs = md_header_splitter.split_text(markdown_content)

for i, doc in enumerate(md_header_docs):
    print(f"Section {i+1}: {repr(doc.page_content[:50])}...")
    print(f"Metadata: {doc.metadata}")
""" 출력 결과
Section 1: '파이썬은 쉽고 강력한 프로그래밍 언어입니다.'...
Metadata: {'Header 1': '파이썬 기초'}
Section 2: '변수는 다음과 같이 선언합니다:\n- name = "홍길동"\n- age = 25'...
Metadata: {'Header 1': '파이썬 기초', 'Header 2': '변수 선언'}
Section 3: '함수는 def 키워드를 사용합니다:'...
Metadata: {'Header 1': '파이썬 기초', 'Header 2': '함수 정의'}
Section 4: '함수는 다음과 같이 호출합니다.'...
Metadata: {'Header 1': '파이썬 기초', 'Header 2': '함수 정의', 'Header 3': '함수 호출'}
"""
```

### HTMLHeaderTextSplitter

HTML 문서를 헤더 태그(```<h1>, <h2>, <h3>```등)를 기준으로 분할한다.

```python
from langchain_text_splitters import HTMLHeaderTextSplitter

html_string = """
<html>
<body>
    <h1>웹 개발 가이드</h1>
    
    <h2>프론트엔드</h2>
    <p>사용자 인터페이스를 담당하는 부분입니다.</p>
    
    <h3>CSS</h3>
    <p>웹 페이지의 스타일을 정의합니다.</p>
</body>
</html>
"""

# 분할할 헤더 태그 지정
headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"), 
    ("h3", "Header 3"),
]

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
html_docs = html_splitter.split_text(html_string)

for i, doc in enumerate(html_docs):
    print(f"Document {i+1}:")
    print(f"Content: {doc.page_content.strip()}")
    print(f"Metadata: {doc.metadata}")
""" 출력 결과
Document 1:
Content: 웹 개발 가이드
Metadata: {'Header 1': '웹 개발 가이드'}
Document 2:
Content: 프론트엔드
Metadata: {'Header 1': '웹 개발 가이드', 'Header 2': '프론트엔드'}
Document 3:
Content: 사용자 인터페이스를 담당하는 부분입니다.
Metadata: {'Header 1': '웹 개발 가이드', 'Header 2': '프론트엔드'}
Document 4:
Content: CSS
Metadata: {'Header 1': '웹 개발 가이드', 'Header 2': '프론트엔드', 'Header 3': 'CSS'}
Document 5:
Content: 웹 페이지의 스타일을 정의합니다.
Metadata: {'Header 1': '웹 개발 가이드', 'Header 2': '프론트엔드', 'Header 3': 'CSS'}
"""
```

### RecursiveJsonSplitter

JSON 데이터를 구조를 유지하면서 적절한 크기로 청킹한다.  

## 궁금했던 내용

Q. MarkdownTextSplitter, HTMLTextSplitter 처럼 전체 스플리터는 없나?
A. MarkdownTextSplitter는 있지만 HTMLTextSplitter는 없다. 핵심은 헤더 중심 스플리터가 더 인기가 많다는 것이다. 이유는 일반적으로 헤더 단위로 논리적 섹션 단위로 분할되어 의미가 보존된다. 그리고 이런 구조화된 문서는 RAG 검색 품질도 크게 향상시킨다.

## 레퍼런스

- 테디노트 CH7 텍스트 분할: <https://wikidocs.net/233776>
