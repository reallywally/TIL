# RAG란 무엇인가

## 핵심 개념 요약

**RAG** 는 Retrieval Augmented Generation의 약어로 **검색 기반 생성** 이라고도 하며, 다음 두 가지 과정을 결합한다.

1. Retrieval (검색)

    - 사용자의 질문(쿼리)에 관련 있는 문서를 **외부 지식 베이스(예: 문서, DB, 파일)** 에서 찾아온다.
    - 일반적으로 벡터 DB(예: FAISS, Weaviate, Pinecone)를 이용해 유사도 기반 검색을 한다.

2. Augmented Generation (보강된 생성)

    - 검색된 문서를 참고하여 LLM이 답변을 **생성(Generation)** 한다.
    - 단순한 질문-답변이 아니라, 근거가 있는 생성형 응답을 한다.

## 작동 흐름

아래와 같은 순서로 하면 LLM이 학습되지 않은 정보도 검색을 통해 보완하여 응답할 수 있습니다.

1. 사용자 질문 입력
2. 질문을 벡터로 임베딩
3. 벡터 DB에서 유사 문서 검색
4. 검색된 문서를 함께 LLM에 입력 (context로 사용)
5. LLM이 문서를 참고하여 답변 생성

## 작동 예시

질문: 파이썬에서 리스트와 튜플의 차이점은?

- 일반 LLM → 훈련 데이터에 없으면 부정확한 답변
- RAG 기반 LLM → Python 공식 문서나 내부 위키에서 관련 내용을 검색해서 정확한 차이점을 알려줌

## 기본 처리 흐름

1. 청킹(Chunking)

    - 원본 문서를 의미있는 단위로 분할. 문맥을 유지하기 위해 일정량 겹치게(overlap) 분할
    - 분할하는 토큰 단위, 청킹 전략(Chunking Strategy)가 중요

2. 임베딩(Embedding)

    - 텍스트로 된 각 청크를 **벡터(Vector)**라는 숫자 리스트로 변환

3. 벡터 저장

    - 생성된 임베딩을 벡터 데이터베이스(Pinecone, Weaviate, Chroma 등)에 저장

4. 검색기(Retriever) 생성

    - 검색 결과에서 선택적으로 Re-ranking하여 검색된 결과들을 재정렬하여 가장 관련성 높은 상위 k개 선별

5. 프롬프트 생성

    - 프롬프트 예시

    ```plaintext
    Context: [검색된 관련 문서들]
    Question: [사용자 문의]
    Instructions: 주어진 컨텍스트만을 사용해서 답변하세요.
    Answer:
    ```

6. 언어 모델(LLM) 생성

    - 모델 선택: GPT-4, Claude, PaLM, HyperCLOVA X 등
    - 안전장치: 환각 방지, 컨텍스트 외 답변 제한

7. 체인 생성 및 실행
    - 체인 실행 순서:
        사용자 쿼리 입력 > Retriever 실행 > Re-ranking (선택적) > 프롬프트 구성 > LLM 호출 > 후처리 (출처 표시, 포맷팅)
    - Parser: 검색된 청크들을 구조화하여 컨텍스트로 준비

## 장점과 한계

1. 장점

    - 최신성: 모델이 학습되지 않은 최신 정보도 검색을 통해 반영 가능
    - 근거 제공: 어디서 가져온 정보인지 근거를 함께 보여줄 수 있음
    - 사내 지식 활용: 기업 내부 문서, 사내 DB 등 비공개 정보 활용 가능
    - 모델 수정 불필요: LLM을 다시 학습시킬 필요 없이 지식만 업데이트하면 됨

2. 한계

    - 잘못된 문서 검색 시 오류 전파: 검색된 문서가 부정확하면 답변도 부정확
    - 검색 품질 의존: 좋은 벡터화/검색 시스템이 필요
    - 문서 수 제한: LLM 입력 토큰 제한 때문에 문서가 많으면 생략되기도 함

## 궁금했던 내용

Q. 체인을 만들때 LLM이 벡터DB를 조회하는건가 아니면 DB에서 조회된 내용을 받기만 하는건가  
A. LLM은 벡터DB를 조회하지 않고 내용을 받는다.
