# 임베딩을 알아보자

## 임베딩이란

청킹(문서분할) 단계에서 생성된 문서들을 **기계가 이해할 수 있는 수치적 형태로 변환**하는 과정이다. 이를 벡터화라고도 한다. 조금 더 깊게 들어가면 벡터화 방법, 유사도 계산 방법도 궁금할 수 있지만 그 부분 매우 어려운 영역이며 ML/AI 연구자, 모델 튜닝 전문가가 담당한다. 이번 포스팅에는 일반적인 RAG 개발자에게 필요한 수준으로만 작성한다.

## 예시

임베딩과 검색에 대한 간단한 예시를 살펴 보자

1. 임베딩 모델을 이용하여 문장을 벡터화

    - "강아지가 공원에서 뛰어다닌다" → [0.2, -0.1, 0.8, ..., 0.3] (768차원)
    - "고양이는 집에서 누워있다" → [0.6, -0.6, 0.1, ..., 0.6] (768차원)

2. 질문도 벡터화
    - "강아지는 어디서 뛰어다니나요?" → [0.2, -0.5, -0.2, ..., 0.4] (768차원)

3. 유사도 계산하여 높은 문장에서 내용 검색
    - 1번: 80% -> 선택!
    - 2번: 30%

## 임베딩 모델 종류

### OpenAIEmbeddings

OpenAI에서 제공하는 API를 이용하여 임베딩하는것으로 **임베딩을 할때마다 비용이 발생**한다. OpenAIEmbeddings 안에서도 3가지 모델이 있는데 예시 코드는 "text-embedding-3-small로 작성하였다.

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

text = "임베딩 테스트를 하기 위한 샘플 문장입니다."

query_result = embeddings.embed_query(text)
```

### CacheBackedEmbeddings

앞서 OpenAIEmbeddings에서 설명하였듯 임베딩은 실행할때 비용이 발생한다. 용량이 작은 문서는 그나마 괜찮은데 용량이 큰 문서를 여러번 임베딩하면 비용도 많이 발생하고 시간도 오래걸려서 효율적이지 못하다. 캐싱 방법으로는 로컬에 파일로 저장하는 LocalFileStore와 메모리에 저장하는 InmemoryByteStore가 있다.

```python
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings

embedding = OpenAIEmbeddings()

# 로컬 파일 저장소 설정
local_store = LocalFileStore("./cache/")

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings=embedding,
    document_embedding_cache=local_store,
    namespace=embedding.model,
) 

# 메모리 저장소 설정
memory_store = InMemoryByteStore()

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    embedding, 
    memory_store,
    namespace=embedding.model
)
```

### HuggingFace Embeddings

```from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings``` 를 이용하면 허깅페이스에 등록된 임베딩 모델을 사용할 수 있다. 예시코드는 생략하고 이런 방법도 있다는 내용만 기억하자

### UpstageEmbeddings
